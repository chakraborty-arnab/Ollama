# Run LLMs locally

Get up and running with large language models quickly and easily.

## Set up
### Download [Ollama](https://ollama.com/)


## Usage
### To run a model, use the following commands on the terminal:
* LLaMA 3
```
ollama run llama
```
* Phi-3
```
ollama run phi-3
```
### Run a script
* Install Requirements
```
pip install -r requirements.txt
```
* Run the script
```
python chat.py
```

## [Inference Demo](https://youtu.be/U-5Hmj0qBQQ?si=2bp321S9VJJKPVyG)

